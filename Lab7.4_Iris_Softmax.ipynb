{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset의 키: \n",
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, pre\n",
      "...\n",
      "타깃의 이름: ['setosa' 'versicolor' 'virginica']\n",
      "특성의 이름: \n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "data의 타입: <class 'numpy.ndarray'>\n",
      "data의 크기: (150, 4)\n",
      "data의 처음 다섯 행:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "target의 타입: <class 'numpy.ndarray'>\n",
      "target의 크기: (150,)\n",
      "타깃:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "print(\"iris_dataset의 키: \\n{}\".format(iris_dataset.keys()))\n",
    "print(iris_dataset['DESCR'][:193] + \"\\n...\")\n",
    "print(\"타깃의 이름: {}\".format(iris_dataset['target_names']))\n",
    "print(\"특성의 이름: \\n{}\".format(iris_dataset['feature_names']))\n",
    "print(\"data의 타입: {}\".format(type(iris_dataset['data'])))\n",
    "print(\"data의 크기: {}\".format(iris_dataset['data'].shape))\n",
    "print(\"data의 처음 다섯 행:\\n{}\".format(iris_dataset['data'][:5]))\n",
    "print(\"target의 타입: {}\".format(type(iris_dataset['target'])))\n",
    "print(\"target의 크기: {}\".format(iris_dataset['target'].shape))\n",
    "print(\"타깃:\\n{}\".format(iris_dataset['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1)\n",
      "X_train 크기: (112, 4)\n",
      "y_train 크기: (112, 1)\n",
      "X_test 크기: (38, 4)\n",
      "y_test 크기: (38, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.float32(iris_dataset['data'])\n",
    "y = np.reshape(iris_dataset['target'],(150,1))\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "print(\"X_train 크기: {}\".format(X_train.shape))\n",
    "print(\"y_train 크기: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_test 크기: {}\".format(X_test.shape))\n",
    "print(\"y_test 크기: {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot: Tensor(\"one_hot_9:0\", shape=(?, 1, 3), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape_7:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 3  \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 4])\n",
    "Y = tf.placeholder(tf.int64, [None, 1])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\", Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model & accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 30\n",
    "batch_size = 40\n",
    "num_iterations = int(112 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 6.757\tAcc: 85.71%\n",
      "Epoch: 0002, Cost: 4.474\tAcc: 85.71%\n",
      "Epoch: 0003, Cost: 3.470\tAcc: 85.71%\n",
      "Epoch: 0004, Cost: 3.217\tAcc: 85.71%\n",
      "Epoch: 0005, Cost: 2.987\tAcc: 85.71%\n",
      "Epoch: 0006, Cost: 2.765\tAcc: 85.71%\n",
      "Epoch: 0007, Cost: 2.543\tAcc: 85.71%\n",
      "Epoch: 0008, Cost: 2.320\tAcc: 85.71%\n",
      "Epoch: 0009, Cost: 2.098\tAcc: 85.71%\n",
      "Epoch: 0010, Cost: 1.882\tAcc: 85.71%\n",
      "Epoch: 0011, Cost: 1.679\tAcc: 85.71%\n",
      "Epoch: 0012, Cost: 1.494\tAcc: 85.71%\n",
      "Epoch: 0013, Cost: 1.330\tAcc: 85.71%\n",
      "Epoch: 0014, Cost: 1.190\tAcc: 85.71%\n",
      "Epoch: 0015, Cost: 1.076\tAcc: 85.71%\n",
      "Epoch: 0016, Cost: 0.988\tAcc: 85.71%\n",
      "Epoch: 0017, Cost: 0.920\tAcc: 85.71%\n",
      "Epoch: 0018, Cost: 0.870\tAcc: 85.71%\n",
      "Epoch: 0019, Cost: 0.833\tAcc: 85.71%\n",
      "Epoch: 0020, Cost: 0.804\tAcc: 85.71%\n",
      "Epoch: 0021, Cost: 0.781\tAcc: 85.71%\n",
      "Epoch: 0022, Cost: 0.763\tAcc: 85.71%\n",
      "Epoch: 0023, Cost: 0.747\tAcc: 85.71%\n",
      "Epoch: 0024, Cost: 0.733\tAcc: 85.71%\n",
      "Epoch: 0025, Cost: 0.721\tAcc: 85.71%\n",
      "Epoch: 0026, Cost: 0.710\tAcc: 85.71%\n",
      "Epoch: 0027, Cost: 0.699\tAcc: 85.71%\n",
      "Epoch: 0028, Cost: 0.690\tAcc: 85.71%\n",
      "Epoch: 0029, Cost: 0.680\tAcc: 85.71%\n",
      "Epoch: 0030, Cost: 0.672\tAcc: 85.71%\n",
      "Learning finished\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[False] Prediction: 2 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "Accuracy:  0.7894737\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: X_train, Y: y_train})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.3f}\\tAcc: {:.2%}\".format(epoch + 1, avg_cost, acc_val))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    pred = sess.run(prediction, feed_dict={X: X_test})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_test.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess, feed_dict={X: X_test, Y: y_test}),)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
