{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression (Kim's example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.set_random_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "X=tf.placeholder(tf.float32,shape=[None,2])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([2,1]),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypo)+(1-Y)*tf.log(1-hypo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypo>0.5,dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5686193\n",
      "1000 0.38491842\n",
      "2000 0.32703802\n",
      "3000 0.28373256\n",
      "4000 0.24963999\n",
      "5000 0.222373\n",
      "6000 0.20023417\n",
      "7000 0.18199019\n",
      "8000 0.16674463\n",
      "9000 0.15384094\n",
      "10000 0.1427932\n",
      "\n",
      "Hypothesis:  [[0.02797207]\n",
      " [0.15491703]\n",
      " [0.2913347 ]\n",
      " [0.7876341 ]\n",
      " [0.94344676]\n",
      " [0.98147607]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    h, c, a = sess.run([hypo, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression (admission data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stats.idre.ucla.edu/stat/data/binary.csv 에서 binary.csv를 다운로드\n",
    "* 아래 binary.csv의 path를 수정해야함\n",
    "* x_data/x_data.max(axis=0)는 x_data상에 최대값으로 나머지 값들을 나눠준 수치. 따라서 모두 1보다 작거나 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(r\"D:\\python_learning\\binary.csv\", delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 1:]\n",
    "x_data = x_data/x_data.max(axis=0)\n",
    "y_data = xy[:, [0]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data를 보게되면 원래 100~600정도하던 첫번째 열이 모두 1보다 작아졌음을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.475 , 0.9025, 0.75  ],\n",
       "       [0.825 , 0.9175, 0.75  ],\n",
       "       [1.    , 1.    , 0.25  ],\n",
       "       ...,\n",
       "       [0.575 , 0.6575, 0.5   ],\n",
       "       [0.875 , 0.9125, 0.5   ],\n",
       "       [0.75  , 0.9725, 0.75  ]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여기서 learnig rate를 어떻게 하느냐에 따라서 결과치가 많이 달라지므로 learning rate와 iteration 횟수를 조정해보고 나오는 수치를 확인할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2531087\n",
      "1000 2.242124\n",
      "2000 2.2311509\n",
      "3000 2.2202253\n",
      "4000 2.2093525\n",
      "5000 2.1985052\n",
      "6000 2.1876743\n",
      "7000 2.1768823\n",
      "8000 2.1661015\n",
      "9000 2.1553326\n",
      "10000 2.144576\n",
      "\n",
      "Hypothesis:  [[0.94551647]\n",
      " [0.96735096]\n",
      " [0.95675445]\n",
      " [0.9729849 ]\n",
      " [0.9651552 ]\n",
      " [0.9600855 ]\n",
      " [0.924165  ]\n",
      " [0.9251795 ]\n",
      " [0.9578444 ]\n",
      " [0.9607739 ]\n",
      " [0.98201406]\n",
      " [0.90961075]\n",
      " [0.953534  ]\n",
      " [0.9560127 ]\n",
      " [0.9482729 ]\n",
      " [0.9533645 ]\n",
      " [0.980996  ]\n",
      " [0.9350406 ]\n",
      " [0.9665021 ]\n",
      " [0.929618  ]\n",
      " [0.953328  ]\n",
      " [0.9561414 ]\n",
      " [0.9694133 ]\n",
      " [0.9748926 ]\n",
      " [0.9619509 ]\n",
      " [0.95470786]\n",
      " [0.93719   ]\n",
      " [0.9688302 ]\n",
      " [0.96264386]\n",
      " [0.9219575 ]\n",
      " [0.97011137]\n",
      " [0.9715628 ]\n",
      " [0.96222264]\n",
      " [0.97584695]\n",
      " [0.89539874]\n",
      " [0.92488337]\n",
      " [0.9293239 ]\n",
      " [0.9532915 ]\n",
      " [0.9376142 ]\n",
      " [0.9518777 ]\n",
      " [0.93829894]\n",
      " [0.94722164]\n",
      " [0.9478879 ]\n",
      " [0.95420647]\n",
      " [0.9551674 ]\n",
      " [0.95172954]\n",
      " [0.9482087 ]\n",
      " [0.964066  ]\n",
      " [0.95717883]\n",
      " [0.9455475 ]\n",
      " [0.9670158 ]\n",
      " [0.9608135 ]\n",
      " [0.9780668 ]\n",
      " [0.9555664 ]\n",
      " [0.9658351 ]\n",
      " [0.9730407 ]\n",
      " [0.9582118 ]\n",
      " [0.9403914 ]\n",
      " [0.93060386]\n",
      " [0.9694133 ]\n",
      " [0.9499177 ]\n",
      " [0.969293  ]\n",
      " [0.96614254]\n",
      " [0.96928954]\n",
      " [0.9639214 ]\n",
      " [0.9508953 ]\n",
      " [0.97881705]\n",
      " [0.9345428 ]\n",
      " [0.9333272 ]\n",
      " [0.95513666]\n",
      " [0.9676453 ]\n",
      " [0.9481609 ]\n",
      " [0.96482605]\n",
      " [0.9518548 ]\n",
      " [0.97749734]\n",
      " [0.9720367 ]\n",
      " [0.95916975]\n",
      " [0.97584695]\n",
      " [0.92291677]\n",
      " [0.94037855]\n",
      " [0.97480273]\n",
      " [0.9491677 ]\n",
      " [0.9340257 ]\n",
      " [0.95501304]\n",
      " [0.9559761 ]\n",
      " [0.9385583 ]\n",
      " [0.9490699 ]\n",
      " [0.95015925]\n",
      " [0.94301045]\n",
      " [0.958297  ]\n",
      " [0.96028847]\n",
      " [0.94760036]\n",
      " [0.9671863 ]\n",
      " [0.9443774 ]\n",
      " [0.9549936 ]\n",
      " [0.9543161 ]\n",
      " [0.97419214]\n",
      " [0.9390613 ]\n",
      " [0.95480037]\n",
      " [0.94525373]\n",
      " [0.9378    ]\n",
      " [0.96173215]\n",
      " [0.95751184]\n",
      " [0.96089363]\n",
      " [0.95801175]\n",
      " [0.95844984]\n",
      " [0.9451142 ]\n",
      " [0.93537843]\n",
      " [0.9423872 ]\n",
      " [0.93807673]\n",
      " [0.97450674]\n",
      " [0.9608979 ]\n",
      " [0.93874145]\n",
      " [0.93148196]\n",
      " [0.9714111 ]\n",
      " [0.9664011 ]\n",
      " [0.93356216]\n",
      " [0.95968723]\n",
      " [0.9549533 ]\n",
      " [0.9358634 ]\n",
      " [0.94450665]\n",
      " [0.93130744]\n",
      " [0.95297366]\n",
      " [0.9521103 ]\n",
      " [0.9715688 ]\n",
      " [0.9684161 ]\n",
      " [0.93433154]\n",
      " [0.9791683 ]\n",
      " [0.94233644]\n",
      " [0.9623083 ]\n",
      " [0.94984996]\n",
      " [0.9490594 ]\n",
      " [0.94778776]\n",
      " [0.95275486]\n",
      " [0.9425235 ]\n",
      " [0.9557962 ]\n",
      " [0.9693353 ]\n",
      " [0.9709964 ]\n",
      " [0.95138615]\n",
      " [0.93467987]\n",
      " [0.9563544 ]\n",
      " [0.9768813 ]\n",
      " [0.97474355]\n",
      " [0.9608319 ]\n",
      " [0.97071785]\n",
      " [0.9441819 ]\n",
      " [0.93766224]\n",
      " [0.9553898 ]\n",
      " [0.9121343 ]\n",
      " [0.9471337 ]\n",
      " [0.9551976 ]\n",
      " [0.9280813 ]\n",
      " [0.95641375]\n",
      " [0.96115196]\n",
      " [0.9649871 ]\n",
      " [0.96845293]\n",
      " [0.93911767]\n",
      " [0.91948086]\n",
      " [0.9552984 ]\n",
      " [0.97026634]\n",
      " [0.9477471 ]\n",
      " [0.9537279 ]\n",
      " [0.9587971 ]\n",
      " [0.93647456]\n",
      " [0.9470791 ]\n",
      " [0.9482729 ]\n",
      " [0.9613986 ]\n",
      " [0.97113323]\n",
      " [0.95831037]\n",
      " [0.9633443 ]\n",
      " [0.94670784]\n",
      " [0.95438313]\n",
      " [0.967683  ]\n",
      " [0.9649961 ]\n",
      " [0.9667268 ]\n",
      " [0.9511883 ]\n",
      " [0.9355371 ]\n",
      " [0.9627069 ]\n",
      " [0.9632145 ]\n",
      " [0.9320151 ]\n",
      " [0.9654172 ]\n",
      " [0.968292  ]\n",
      " [0.9612007 ]\n",
      " [0.9471614 ]\n",
      " [0.96316814]\n",
      " [0.9658041 ]\n",
      " [0.9605404 ]\n",
      " [0.9440025 ]\n",
      " [0.9434415 ]\n",
      " [0.93942046]\n",
      " [0.95008147]\n",
      " [0.9742552 ]\n",
      " [0.96655893]\n",
      " [0.95899177]\n",
      " [0.95009184]\n",
      " [0.94726264]\n",
      " [0.96454436]\n",
      " [0.95845246]\n",
      " [0.96339446]\n",
      " [0.97217786]\n",
      " [0.97342145]\n",
      " [0.9463614 ]\n",
      " [0.9482729 ]\n",
      " [0.96353525]\n",
      " [0.937322  ]\n",
      " [0.97424006]\n",
      " [0.94874865]\n",
      " [0.93952906]\n",
      " [0.95650285]\n",
      " [0.94848746]\n",
      " [0.9779751 ]\n",
      " [0.9450463 ]\n",
      " [0.9307203 ]\n",
      " [0.9647664 ]\n",
      " [0.9506956 ]\n",
      " [0.96375746]\n",
      " [0.88848567]\n",
      " [0.91730285]\n",
      " [0.9098939 ]\n",
      " [0.94726264]\n",
      " [0.9572078 ]\n",
      " [0.969877  ]\n",
      " [0.9133807 ]\n",
      " [0.97400415]\n",
      " [0.9623574 ]\n",
      " [0.9700365 ]\n",
      " [0.9525571 ]\n",
      " [0.9668112 ]\n",
      " [0.9379113 ]\n",
      " [0.9594899 ]\n",
      " [0.9702696 ]\n",
      " [0.96159697]\n",
      " [0.925531  ]\n",
      " [0.9424199 ]\n",
      " [0.95390123]\n",
      " [0.9490303 ]\n",
      " [0.9552984 ]\n",
      " [0.9424701 ]\n",
      " [0.96351993]\n",
      " [0.96870923]\n",
      " [0.9272071 ]\n",
      " [0.92711794]\n",
      " [0.9652872 ]\n",
      " [0.9501877 ]\n",
      " [0.9221031 ]\n",
      " [0.97554326]\n",
      " [0.95598745]\n",
      " [0.9310479 ]\n",
      " [0.9683875 ]\n",
      " [0.96642065]\n",
      " [0.9743854 ]\n",
      " [0.9720562 ]\n",
      " [0.9464141 ]\n",
      " [0.9691477 ]\n",
      " [0.97852   ]\n",
      " [0.96462095]\n",
      " [0.9411322 ]\n",
      " [0.9662158 ]\n",
      " [0.9427661 ]\n",
      " [0.955636  ]\n",
      " [0.9545896 ]\n",
      " [0.93086493]\n",
      " [0.95509493]\n",
      " [0.9662158 ]\n",
      " [0.959234  ]\n",
      " [0.94350314]\n",
      " [0.96895236]\n",
      " [0.96344453]\n",
      " [0.95670027]\n",
      " [0.9509951 ]\n",
      " [0.956473  ]\n",
      " [0.9574982 ]\n",
      " [0.9579214 ]\n",
      " [0.93915796]\n",
      " [0.93969715]\n",
      " [0.94668   ]\n",
      " [0.95377886]\n",
      " [0.9323465 ]\n",
      " [0.97422236]\n",
      " [0.95312285]\n",
      " [0.9579545 ]\n",
      " [0.94091386]\n",
      " [0.97487795]\n",
      " [0.9659596 ]\n",
      " [0.9330306 ]\n",
      " [0.9714144 ]\n",
      " [0.9519222 ]\n",
      " [0.9757346 ]\n",
      " [0.9797485 ]\n",
      " [0.95426404]\n",
      " [0.95171416]\n",
      " [0.96173394]\n",
      " [0.9580931 ]\n",
      " [0.95657754]\n",
      " [0.90794116]\n",
      " [0.9554596 ]\n",
      " [0.92594075]\n",
      " [0.932533  ]\n",
      " [0.94468707]\n",
      " [0.9696201 ]\n",
      " [0.95245564]\n",
      " [0.96703506]\n",
      " [0.925866  ]\n",
      " [0.959664  ]\n",
      " [0.9199787 ]\n",
      " [0.97095954]\n",
      " [0.9234215 ]\n",
      " [0.948557  ]\n",
      " [0.9418707 ]\n",
      " [0.9466969 ]\n",
      " [0.9625908 ]\n",
      " [0.9563795 ]\n",
      " [0.9677974 ]\n",
      " [0.9684412 ]\n",
      " [0.9687624 ]\n",
      " [0.908275  ]\n",
      " [0.9161166 ]\n",
      " [0.98034877]\n",
      " [0.96633905]\n",
      " [0.92452097]\n",
      " [0.94966215]\n",
      " [0.93695796]\n",
      " [0.9642627 ]\n",
      " [0.9239311 ]\n",
      " [0.9520079 ]\n",
      " [0.9456708 ]\n",
      " [0.95580745]\n",
      " [0.94647515]\n",
      " [0.94736373]\n",
      " [0.9638684 ]\n",
      " [0.9730407 ]\n",
      " [0.96698964]\n",
      " [0.9032874 ]\n",
      " [0.9595581 ]\n",
      " [0.95295644]\n",
      " [0.93802243]\n",
      " [0.9548502 ]\n",
      " [0.9715886 ]\n",
      " [0.92372274]\n",
      " [0.9664402 ]\n",
      " [0.96532595]\n",
      " [0.9550245 ]\n",
      " [0.96859634]\n",
      " [0.94534117]\n",
      " [0.9560613 ]\n",
      " [0.9524336 ]\n",
      " [0.94927585]\n",
      " [0.95053375]\n",
      " [0.92789114]\n",
      " [0.94763595]\n",
      " [0.96643645]\n",
      " [0.9637154 ]\n",
      " [0.9593046 ]\n",
      " [0.9585717 ]\n",
      " [0.9467324 ]\n",
      " [0.94542843]\n",
      " [0.95713574]\n",
      " [0.94521916]\n",
      " [0.9609696 ]\n",
      " [0.9718038 ]\n",
      " [0.9289238 ]\n",
      " [0.9265799 ]\n",
      " [0.9547744 ]\n",
      " [0.935771  ]\n",
      " [0.9278687 ]\n",
      " [0.94900674]\n",
      " [0.9482144 ]\n",
      " [0.9646975 ]\n",
      " [0.93602014]\n",
      " [0.96714103]\n",
      " [0.94666064]\n",
      " [0.96890604]\n",
      " [0.93377876]\n",
      " [0.93514943]\n",
      " [0.948597  ]\n",
      " [0.97000486]\n",
      " [0.95287764]\n",
      " [0.9676349 ]\n",
      " [0.9634869 ]\n",
      " [0.93842876]\n",
      " [0.9593001 ]\n",
      " [0.94461274]\n",
      " [0.9447614 ]\n",
      " [0.94445753]\n",
      " [0.93085086]\n",
      " [0.9040307 ]\n",
      " [0.96321046]\n",
      " [0.9475054 ]\n",
      " [0.9516109 ]\n",
      " [0.95379066]\n",
      " [0.9631231 ]\n",
      " [0.95760924]\n",
      " [0.962119  ]\n",
      " [0.95363843]\n",
      " [0.9551394 ]\n",
      " [0.95518607]\n",
      " [0.95734876]\n",
      " [0.92848575]\n",
      " [0.9593001 ]\n",
      " [0.964677  ]] \n",
      "Correct (Y):  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.3175\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
